            +--------------------+
            |        CS 140      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+
                   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Woo Hyun Jin <whjin@stanford.edu>
Joon Yeong Kim <kim64@stanford.edu>
Anand Madhavan <manand@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

***************************************************************************
*****
/* Sorted list of semaphores for threads 
   put to sleep */
static struct list wait_semas;

A sorted list of semaphores for threads put to sleep for timer interrupt
to check for threads that need to wake up.

*****
/* A global lock to synchronize insertion 
   to wait_semas */
static struct lock wait_semas_lock;

A lock for wait_semas list access to synchronize insertion into the list.

*****
/* Semaphore element for sleeping threads. */
struct sleep_sema_elem
  {
    struct list_elem elem;      /* for using the elements in wait_semas list */
    int wakeup_tick;		/* wake me up when this tick occurs */
    struct semaphore sema;  	/* semaphore to signal the thread to 
    	   	     		   wake up at wakeup_tick */
  };

A struct exclusively for semaphores used for putting threads to sleep.
Contains a semaphore element and the time (tick) it is supposed to 
wake up.
***************************************************************************

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

***************************************************************************
In our design, we start with a list of semaphores, wait_semas, for threads 
that were to be put to sleep with each of their 'time to be woken up' or 
'wakeup_tick' recorded.

When timer_sleep is called, a semaphore is created and the tick it is 
supposed to be woken up at is recorded with the semaphore and combined as 
a sleep_sema elem. 

This list is looked at in the timer_interrupt to determine which threads 
are to be woken up.
***************************************************************************


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

***************************************************************************
Our initial design for the sleep_sema_elem struct recorded the number of 
ticks left until wake up, which required decrementing of the count for
each thread every time the timer interrupt is called.

The amount of time spent in the timer interrupt was minimized by recording
the tick to wake up instead of ticks left. This is put in the element and 
the element is inserted into wait_semas ordered by increasing wakeup_tick.

This helps in an efficient implementation of the 'waking up' of the threads. 
The timer interrupt can now just go through the elements in increasing order
of wakeup_tick, waking each thread up that was to be woken up at the current 
(or past) tick. It wakes each of them up until it reaches a thread that is 
to be woken up in the future (all subsequent threads can be assumed to be 
required to woken up only in the future since the elements of the list are 
ordered by this wake up time) and we can perform an efficient early exit.

***************************************************************************

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

***************************************************************************
As the description for the global lock wait_semas_lock describes, the lock
is used to lock the access to the global semaphore list, wait_semas.

When multiple threads call timer_sleep() simultaneously, only a single
thread acquires wait_semas_lock each time it inserts its sleep_sema_elem
into wait_semas list. Since the semaphores are inserted in order a single
element at a time, the threads are always woken up in the correct order.

We keep the critical section minimal by surrounding just the 
list_insert_ordered function with the lock acquisition and release.

Thus the race condition is resolved.
***************************************************************************

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

***************************************************************************
Race condition occurs when access to the list of sleeping threads, 
wait_semas, is not controlled properly. In our design, the frontmost
element of wait_semas list is checked each time the timer interrupt occurs
and if the first N thread's sleep time have expired, it wakes the threads 
by upping the corresponding semaphore and removing the element from 
wait_semas list. 

We use lock_try_acquire to limit the access to wait_semas list for cases 
where the timer interrupt is called within the critical section we have 
locked inside timer_sleep (), i.e. where the sleep_sema_elem is inserted
into wait_semas list. If lock_try_acquire () fails, it skips checking and
accessing the wait_semas, thus preserving the previous state of the list.

Hence, race condition does not occur from this scenario.
***************************************************************************

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

***************************************************************************
As discussed in Question A3, our initial design recorded remaining ticks 
to wake-up instead of wake-up-tick. The current design is much better than
our previous version because it reduced the number of instructions the 
timer interrupt has to handle tremendously, from linear to a constant. 

The list of threads put to sleep, wait_semas, could have been replaced by 
a more efficient data structure such as a heap since it involves
insertion in order which takes O(log N) time. However, because we were 
provided with linked list structure which we assume has been tested and tried
over time, and hence more reliable (and simple) than the code we may have 
produced, we chose code robustness over efficiency for this data structure.

***************************************************************************

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

***************************************************************************
*****
struct lock 
  {
    struct thread *holder;      /* Thread holding lock (for debugging). */
    struct semaphore semaphore; /* Binary semaphore controlling access. */
    struct list_elem elem;      /* List elem. */
    struct semaphore acquire_semaphore; /* Semaphore for controlling
					   acquire */
  };

Each time a lock is acquired we use the above acquire_semaphore to control access
to each lock.

struct semaphore acquire_semaphore;

A semaphore that acts as a lock for priority donation to make a full 
donation propagation to complete before another priority donation can 
start.

*****
struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    int orig_priority;                  /* Priority that is originally set. */

    int mlfqs_nice;                     /* nice value for MLFQS */
    int mlfqs_recent_cpu;               /* recent_cpu for MLFQS */

    struct list acquired_locks;         /* locks that this thread has acquired. */
    struct lock *blocking_lock;         /* lock that this thread has been blocked by */

    struct list_elem allelem;           /* List element for all threads list. */

    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif

    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
  };

orig_priority, acquired_locks, blocking_lock were added for impelementing 
priority scheduling, priority donation and priority propagation. 

orig_priority is the original priority set by external calls to set_priority. 
acquired_locks keeps tracks of which locks this thread currently has acquired
and blocking_lock is a pointer to the lock that the threaad is blocked on. 

MLFQS related structures mlfqs_nice, mlfqs_recent_cpu were added (for Part C). 

struct list acquired_locks;

A list of locks a thread has acquired to compute its donated priority
after a lock has been released. Details in diagram for B2.

struct lock *blocking_lock;

The lock that a thread is blocked on so priority is recursively
donated to the next level if current thread is blocked by a lock.
***************************************************************************

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

***************************************************************************
The data structure is explained along with diagrams in the file 
Prof1_B2_priority_donation_diagram.png in the same directory.
***************************************************************************

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

***************************************************************************
Semaphore: semaphore has a list of waiters that are threads in its 
data structure. As semaphore's value comes down to zero and threads
that reach the semaphore waits for "sema_up"s we add each thread
as they reach the semaphore's "sema_down" to the list of waiters
using the function "list_insert_ordered" provided with our 
comparison function that compares the priorities of the threads.
We have stipulated in the comparison function that threads 
are ordered from highest priority to lowest in the waiters list,
so that when some other thread "sema_up"s this semaphore, our 
usage of "list_pop_front" function will effectively wake up the 
thread of the highest priority in the waiters list.

Lock: lock is basically a semaphore with the value of 1. Hence, the 
lock data structure contains a semaphore and this semaphore will 
have a list of threads that are waiting for the next "sema_up". 
Again, threads that reach the semaphore of the lock when the
semaphore's value is zero is added to the list of threads (waiters)
with the function "list_insert_ordered" with the comparison function
that compares priorities of threads. Upon a "sema_up" the thread
at the very front of the list will be woken up by the function 
"list_pop_front" and "thread_unblock". 

Note: however, we cannot ensure the "sortedness" of the waiters list
when priority donation is active. Therefore, we have arranged it so
that the list of waiters is sorted just before the "list_pop_front"
function is called to fetch the thread with the highest priority in 
waiters' list. The issue of synchronization comes into attention here
because after the list is sorted through "list_sort" the CPU can 
switch context and other priority donations might happen that involves
threads within the waiters' list. Hence, we use the existing protected
section in "thread_unblock" (where interrupts are disabled) to modify
and sort our list.

Condition variable: Internally, a wait on a condition variable is 
also implemented by a semaphore. Like a lock, the semaphore contains
a sorted list of threads ordered by priorities. When cond_signal ()
is called, the thread with the highest priority is popped from the 
waiters list of the semaphore. 

Thus, all the synchronization schemes rely on the sortedness of the 
semaphore's waiter list.
***************************************************************************

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

***************************************************************************
We have noted in the assignment description that we only have to 
deal with priority donations relating to locks.

The current thread will first have to pass through three ASSERT 
statements and "sema_down" the acquire_semaphore in the lock 
structure. Acquire_semaphore is a synchronization measure we have 
added to the lock data structure so that priority donations that 
propagate from this lock happens atomically. This will ensure that 
donation of priorities happen in a synchronized manner. Then the 
current thread will go into the function "lock_propagate_donation" and 
donate priority to the holder of this lock, say lock A, if it has 
higher priority than the holder of this lock. 

If the holder lock A is also blocked by some other lock, say lock B, 
and if priority of the holder of lock A is higher than 
that of holder of lock B, priority is donated from the holder of lock A
to the holder of lock B. The same process will happen recursively 
until the holder of a lock down the chain is in a blocked status. 
This recursive process handles nested priority donation well.   
***************************************************************************

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

***************************************************************************
We have added three new variables to the data structures of thread
and lock so that a lock knows which thread is holding it, a thread
has a list of locks that it has acquired, and a thread remembers the
original priority it had before any priority donations happened.

When lock_release is called by the current thread, we first set the
holder of this lock to NULL and remove this lock from the list of 
locks acquired by the current thread. Then by iterating through the
list of locks still acquired by the current thread, we determine 
the consequent priority value for the current thread after release 
of this lock. This process is to make sure that since priorities could
have been donated by waiters of other locks that the current thread 
holds, even after the current thread releases this lock, it cannot 
return straight to the original priority (before any donations) it had
but it has to return to the highest priority donated by waiters of 
other locks it holds. 

After priority value has been determined, we call "sema_up" function 
for this lock's semaphore, which will first disable interrupts, sort 
the list of waiters so that the waiter with the highest priority will 
be at the very front of the list, call "list_pop_front" to fetch the 
thread with highest priority, and then unblock the thread. At this 
moment, the CPU is likely to switch context to the thread just unblocked.
***************************************************************************

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

***************************************************************************
There indeed exists a potential race in the "thread_set_priority".
Our version of "thread_set_priority" contains an if-statement that
sets the actual priority to new_priority (argument of the function)
only if new_priority is higher than the current actual priority of the
current thread. The CPU can switch context right at the moment when 
thread A passes the test conditions of the if-statement (line 551) 
and enters the if-statement body but has not executed any statements.
Then, the thread to which the CPU has changed context, say thread B, 
can donate priority to thread A and then change context back to 
thread A. Then, the conditions of the if-statement does not hold any 
more but we have already entered the body of the if-statement with 
thread A. Thread A's actual priority might be higher than the 
new_priority but it is still set to the new_priority where it should 
not have. Therefore, we can see that such race condition can 
perturb the priorities of threads if "thread_set_priority" is not 
synchronized. 

We believe that a lock cannot be used to synchronize "thread_set_
priority" because it could lead to a deadlock situation. If we create
a lock for priority values of a thread in order to avoid above race
condition, we can find ourselves in the below undesirable situation.
Thread A calls "thread_set_priority" and acquires the lock for 
its priority values. In the middle of the function, CPU switches context
to thread B, a thread with higher priority. Thread B is blocked by some 
other lock that thread A holds and tries to donate priority to thread A
but it is blocked again by the lock for thread A's priority values.
Priority donation cannot happen and many other threads exist in 
the system so CPU does not switch to thread A quickly. We find ourselves
in a deadlock situation. 

Our solution to this problem is disabling interrupts when we enter
"thread_set_priority" and returning it to old level when we exit. 
This way, CPU cannot switch context until the "thread_set_priority"
function finishes.  
***************************************************************************

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

***************************************************************************
We considered other designs in the process of implementing priority
scheduling. However, we chose this design because of several reasons.

We considered using a binary tree in place of the "ready_list" of threads
because it would yield faster for "list_insert_ordered" and we call
"list_insert_ordered" frequently. However, we decided not to use binary
tree for the "ready_list" because there could also be frequent calls to 
priority donation and multiple, complex calls to priority donation will
require restructuring of the binary tree and this would be costly. The
cost of frequent restructuring of the binary tree would exceed the benefit
of it being faster for "list_insert_ordered" and therefore, we chose
to move forward with "ready_list" implemented with "list.h" in given
files.
***************************************************************************

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

***************************************************************************
*****
/* System-wide load average for MLFQS */
static int mlfqs_load_avg;

Global load average variable, which is a real number representation, that
is updated every second.

*****
/* Ready list (Queue) for each priority */
static struct list mlfqs_ready_list[PRI_MAX+1];

The 64 ready queues for 64 different priorities ranging from 0 to 63.

*****
struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    int orig_priority;                  /* Priority that is originally set. */

    int mlfqs_nice;                     /* nice value for MLFQS */
    int mlfqs_recent_cpu;               /* recent_cpu for MLFQS */

    struct list acquired_locks;         /* locks that this thread has acquired. */
    struct lock *blocking_lock;         /* lock that this thread has been blocked by */

    struct list_elem allelem;           /* List element for all threads list. */

    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif

    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
  };

int mlfqs_nice;

Nice value of the thread ranging from -20 to 20.


int mlfqs_recent_cpu;

Recent CPU usage variable, which is a real number representation, updated
each second.
***************************************************************************

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

Note that since we are only interested in the 36 ticks below, and since 
we update the recent_cpu with load_avg information only every second, 
we only need to increment the recent_cpu of the running thread below.
We also assume that no threads are blocked on any resource. Also since 
priority is updated only every fourth clock tick, there is no reason for 
a context switch and we can assume that a thread continues to run for 
all the four ticks.

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59	     A 
 4	4   0	0  62  61  59	     A	
 8	8   0	0  61  61  59	     B
12	8   4	0  61  60  59	     A
16     12   4	0  60  60  59	     B
20     12   8	0  60  59  59	     A
24     16   8	0  59  59  59	     C
28     16   8	4  59  59  58	     B
32     16  12	4  59  58  58	     A
36     20  12	4  58  58  58	     C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

***************************************************************************
It is ambiguous if the recent_cpu value should be calculated at
initialization or not. Right now recent_cpu is recalculated once every
second. This is implemented by computing recent_cpu when
(timer_ticks () % TIMER_FREQ == 0). So, it is possible that one of the 
three threads be initialized at the moment when this condition is true. 
Then recent_cpu of that thread is computed using the load average and its
thread's nice value. For this scenario, if the timer interrupt occurred
right after thread C was initialized, the above table changes like the
following:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   2  63  61  58	     A 
 4	4   0	2  62  61  58	     A	
 8	8   0	2  61  61  58	     B
12	8   4	2  61  60  58	     A
16     12   4	2  60  60  58	     B
20     12   8	2  60  59  58	     A
24     16   8	2  59  59  58	     B
28     16  12 	2  59  58  58	     A
32     20  12	2  58  58  58	     C
36     20  12	6  58  58  57	     B

TODO: Do we want to resolve this issue? I don't think it is that
necessary. There is something fishy about if (recent_cpu != 0), 
and adding a flag variable just doesn't seem right.
***************************************************************************

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

***************************************************************************

The number of instructions that an interrupt can handle is limited 
because the timer interrupt has to be executed every tick. Thus it is crucial 
to keep the number of instructions an interrupt executes to a minimum.
Expensive computations should be avoided in interrupt handling code and 
moved out. 

Since thread instructions are executed in between these timer interrupts, 
by reducing the number of instructions that the interrupt executes to a 
minimum, the performance gain is tremendous over long period of time, 
since the number of thread instructions executed in total increases by
the number of instructions reduced in the timer interrupt multiplied by
a very large number.
 
***************************************************************************

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

***************************************************************************

Despite reducing the number of updates through the entire list of threads
to just once per 4 cycles and the updates on recent_cpu's to just once per
second, our interrupt handler code currently still goes through the entire
list of threads. This can get expensive if there are a large number of threads.
 
One way to alleviate this problem might be to do the load_avg and the 
recent_cpu updates outside of the handler and instead in the context switching
portion (either in thread_block or thread_unblock). However depending on the
number of threads there might be more context switches taking place between
two ticks that this in itself might get more prohibitive than doing it in the
interrupt handler. One way to deal with these two extremes might be dynamically
decide where to update based on number of ready threads and the total threads. 

If there are not that many threads in total (based on an empirically tuned 
cutoff), we could just update it in the interrupt handler. If there are many 
threads in total and not that many threads waiting, we know there are not going
to be too many context switches and hence we can just update them in thread_block/
thread_unblock.

***************************************************************************

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

***************************************************************************
We implemented fixed-point math in lib/reals.h and lib/reals.c. We were able
to write separate tests for these and verify our implementation independent of 
the rest of the code. We integrated the simple function calls into our code
by including the header. The implementation included some non-obvious bit 
manipulation that would have decreased the clarity of our code had we just
implemented it in the thread code. So it was important to abstract it out
 into a simple set of functions. Further, changing the internal fixed point
representation (eg: exponent from 14 bits to say 16 bits) would have
unnecessarily altered thread code when in fact it is change to the common
representation of reals. Hence we used a separate set of functions.

We could additionally have made these functions inline in lib/reals.h but 
do not expect it to affect performance for this project purposes, so left
the implementation as a separate c file for this project.

***************************************************************************

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

